{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sum_det(x):\n",
    "    v = tf.reshape(x, [1, -1])\n",
    "    return tf.reshape(tf.matmul(v, tf.ones_like(v), transpose_b=True), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialLogisticModel(object):\n",
    "    \"\"\"A model for doing adversarial training of logistic models.\"\"\"\n",
    "    def __init__(self,  \n",
    "                scope_name,\n",
    "                sess,\n",
    "                hyperparameters,\n",
    "                seed=None,\n",
    "                debias=True):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.adversary_loss_weight = hyperparameters['adversary_loss_weight']\n",
    "        self.num_epochs = hyperparameters[\"num_epochs\"]\n",
    "        self.batch_size = hyperparameters[\"batch_size\"]\n",
    "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
    "        self.debias = debias\n",
    "        \n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def predictor_model(self, features, features_dim):\n",
    "        with tf.variable_scope(\"predictor_model\"):\n",
    "            W1 = tf.get_variable('W1', [self.features_dim, 1], initializer=tf.glorot_uniform_initializer(seed=self.seed))\n",
    "            b1 = tf.Variable(tf.zeros(shape=[1]), name='b1')\n",
    "\n",
    "            pred_logits = tf.matmul(features, W1) + b1\n",
    "            pred_labels = tf.sigmoid(pred_logits)\n",
    "\n",
    "        return pred_labels, pred_logits\n",
    "\n",
    "    def adversarial_model(self, pred_logits, true_labels):\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1], initializer=tf.glorot_uniform_initializer(seed=self.seed))\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logits = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_labels = tf.sigmoid(pred_protected_attribute_logits)\n",
    "\n",
    "        return pred_protected_attribute_labels, pred_protected_attribute_logits\n",
    "\n",
    "\n",
    "    def fit(self, features, labels, protect):\n",
    "        if self.seed is not None:\n",
    "            tf.random.set_random_seed(self.seed)\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self.predictor_model(self.features_ph, self.features_dim)\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self.adversarial_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = self.learning_rate\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "            predictor_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "#             # To deal with local minima, we increase alpha \n",
    "#             X = tf.cast(global_step, tf.float32)\n",
    "#             adversary_loss_weight = tf.math.sqrt(X)\n",
    "            \n",
    "            if self.debias:\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            predictor_vars = [var for var in tf.trainable_variables() if 'predictor_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables() if 'adversary_model' in var.name]\n",
    "                # Compute gradient of adversarial loss with respect to predictor variables\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss, var_list=predictor_vars, gate_gradients=adversary_opt.GATE_GRAPH)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "            predictor_grads = []\n",
    "            gradients = predictor_opt.compute_gradients(pred_labels_loss, var_list=predictor_vars, gate_gradients=predictor_opt.GATE_GRAPH)\n",
    "            for (grad,var) in gradients:\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= reduce_sum_det(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                predictor_grads.append((grad, var))\n",
    "            # Update predictor parameters\n",
    "            predictor_minimizer = predictor_opt.apply_gradients(predictor_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars, global_step=global_step, gate_gradients=adversary_opt.GATE_GRAPH)\n",
    "\n",
    "            tf.random.set_random_seed(self.seed)\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "            \n",
    "\n",
    "            # Begin training\n",
    "            epoch_losses = []\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples)\n",
    "                # shuffled_ids = np.array(range(num_train_samples))\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = features[batch_ids]\n",
    "                    batch_labels = np.reshape(labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(protect[batch_ids], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                        self.true_labels_ph: batch_labels,\n",
    "                                        self.protected_attributes_ph: batch_protected_attributes\n",
    "                                        }\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([predictor_minimizer, adversary_minimizer,\n",
    "                                       pred_labels_loss, pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch predictor loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run([predictor_minimizer, pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "                            \n",
    "                epoch_losses.append(pred_labels_loss_value)\n",
    "        return self, epoch_losses\n",
    "\n",
    "    def predict(self, features, labels, protect):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair classifier learned.\n",
    "        Args:\n",
    "            features\n",
    "            labels\n",
    "            protect\n",
    "        Returns:\n",
    "            predictions\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = features[batch_ids]\n",
    "            batch_labels = np.reshape(labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(protect[batch_ids], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               }\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        true_labels = list(labels.reshape(1, -1)[0])\n",
    "        pred_labels = np.array([1 if i > 0.5 else 0 for i in pred_labels]).reshape(-1,1)\n",
    "        return pred_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    return pd.read_csv(filepath) \n",
    "\n",
    "def split_train_test(compas_df, train=0.75):\n",
    "    np.random.seed(seed=1)\n",
    "    shuffled = np.random.permutation(compas_df.index)\n",
    "    n_train = int(len(shuffled) * train)\n",
    "    i_train, i_test = shuffled[:n_train], shuffled[n_train:]\n",
    "    return compas_df.loc[i_train], compas_df.loc[i_test]\n",
    "\n",
    "\n",
    "def process_data(df, label_column, protect_column):\n",
    "    feature_columns = set(df.columns) - {label_column, protect_column}\n",
    "    features = df[list(feature_columns)].values\n",
    "    labels = df[label_column].values.reshape(-1,1)\n",
    "    protect = df[protect_column].values.reshape(-1,1)\n",
    "    return (features, labels, protect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = '../data/Compas Dataset/processed_compas.csv'\n",
    "# compas_df = pd.read_csv(filepath)\n",
    "# label_column = \"is_recid\"\n",
    "# protect_column = \"race_African-American\"\n",
    "\n",
    "# train_df, test_df = split_train_test(compas_df)\n",
    "# train_features, train_labels, train_protect = process_data(train_df, label_column, protect_column)\n",
    "# test_features, test_labels, test_protect = process_data(test_df, label_column, protect_column)\n",
    "\n",
    "# train_df[:10]['jail_stay_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_confusion_matrix(true_labels, pred_labels, protect, protect_group):\n",
    "#     indices = np.where(protect == protect_group)\n",
    "#     group_pred_labels = pred_labels[indices]\n",
    "#     group_true_labels = true_labels[indices]\n",
    "\n",
    "#     return confusion_matrix(group_true_labels, group_pred_labels)\n",
    "\n",
    "# def false_positive_rate(group_confusion_matrix):\n",
    "#     return group_confusion_matrix[0][1]/np.sum(group_confusion_matrix[0,:])\n",
    "\n",
    "# def true_negative_rate(group_confusion_matrix):\n",
    "#     return 1 - false_positive_rate(group_confusion_matrix)\n",
    "\n",
    "# def false_negative_rate(group_confusion_matrix):\n",
    "#     return group_confusion_matrix[1][0]/np.sum(group_confusion_matrix[1,:])\n",
    "\n",
    "# def true_positive_rate(group_confusion_matrix):\n",
    "#     return 1 - false_negative_rate(group_confusion_matrix)\n",
    "\n",
    "# def false_positive_rate_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "#     return false_positive_rate(confusion_matrix_1) - false_positive_rate(confusion_matrix_2) \n",
    "\n",
    "# def true_positive_rate_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "#     return true_positive_rate(confusion_matrix_1) - true_positive_rate(confusion_matrix_2) \n",
    "\n",
    "# def false_negative_rate_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "#     return false_negative_rate(confusion_matrix_1) - false_negative_rate(confusion_matrix_2)\n",
    "\n",
    "# def average_odds_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "#     fpr_difference = false_positive_rate_difference(confusion_matrix_1, confusion_matrix_2)\n",
    "#     tpr_difference = true_positive_rate_difference(confusion_matrix_1, confusion_matrix_2)\n",
    "#     return 0.5*(fpr_difference + tpr_difference)\n",
    "\n",
    "# def frac_predicted_positive(confusion_matrix):\n",
    "#     return np.sum(confusion_matrix[:,1])/np.sum(confusion_matrix)\n",
    "\n",
    "# def statistical_parity_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "#     frac_prediced_positive_1 = frac_predicted_positive(confusion_matrix_1)\n",
    "#     frac_prediced_positive_2 = frac_predicted_positive(confusion_matrix_2)\n",
    "#     return frac_prediced_positive_1 - frac_prediced_positive_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
