{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from load_dataset.ipynb\n",
      "importing Jupyter notebook from adversarial_debiasing_model.ipynb\n",
      "importing Jupyter notebook from fairness_metrics.ipynb\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import current\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = current.read(f, 'json')\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        try:\n",
    "            for cell in nb.worksheets[0].cells:\n",
    "                if cell.cell_type == 'code' and cell.language == 'python':\n",
    "                    # transform the input to executable Python\n",
    "                    code = self.shell.input_transformer_manager.transform_cell(cell.input)\n",
    "                    # run the code in themodule\n",
    "                    exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "\n",
    "from load_dataset import *\n",
    "from adversarial_debiasing_model import *\n",
    "from fairness_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "scaled\n",
      "[[1477 1736]\n",
      " [1078 2135]]  balanced is_recid race_African-American\n",
      "balanced\n"
     ]
    }
   ],
   "source": [
    "###### compas dataset\n",
    "filepath = \"../../data/Compas Dataset/processed_compas.csv\"\n",
    "label, protect = \"is_recid\", \"race_African-American\"\n",
    "\n",
    "balanced = {\"label_only\":True,\"downsample\":True}\n",
    "num_proxy_to_remove = 0\n",
    "train_dataset, test_dataset = train_test_dataset(filepath, label, protect, \n",
    "                                                 is_scaled=True,\n",
    "                                                 num_proxy_to_remove=num_proxy_to_remove,\n",
    "                                                 balanced=balanced\n",
    "                                                )\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, worker_init_fn=np.random.seed(1))\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False, worker_init_fn=np.random.seed(1))\n",
    "\n",
    "train_features, train_labels, train_protect = train_loader.dataset.features, train_loader.dataset.label, train_loader.dataset.protect\n",
    "test_features, test_labels, test_protect = test_loader.dataset.features, test_loader.dataset.label, test_loader.dataset.protect\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "epoch 0; iter: 0; batch classifier loss: 0.686575\n",
      "epoch 1; iter: 0; batch classifier loss: 0.685691\n",
      "epoch 2; iter: 0; batch classifier loss: 0.680936\n",
      "epoch 3; iter: 0; batch classifier loss: 0.650488\n",
      "epoch 4; iter: 0; batch classifier loss: 0.646870\n",
      "epoch 5; iter: 0; batch classifier loss: 0.647045\n",
      "epoch 6; iter: 0; batch classifier loss: 0.642570\n",
      "epoch 7; iter: 0; batch classifier loss: 0.630618\n",
      "epoch 8; iter: 0; batch classifier loss: 0.621527\n",
      "epoch 9; iter: 0; batch classifier loss: 0.644020\n",
      "epoch 10; iter: 0; batch classifier loss: 0.616064\n",
      "epoch 11; iter: 0; batch classifier loss: 0.626088\n",
      "epoch 12; iter: 0; batch classifier loss: 0.629445\n",
      "epoch 13; iter: 0; batch classifier loss: 0.631070\n",
      "epoch 14; iter: 0; batch classifier loss: 0.639709\n",
      "epoch 15; iter: 0; batch classifier loss: 0.594149\n",
      "epoch 16; iter: 0; batch classifier loss: 0.598540\n",
      "epoch 17; iter: 0; batch classifier loss: 0.592634\n",
      "epoch 18; iter: 0; batch classifier loss: 0.599243\n",
      "epoch 19; iter: 0; batch classifier loss: 0.611806\n",
      "epoch 20; iter: 0; batch classifier loss: 0.584270\n",
      "epoch 21; iter: 0; batch classifier loss: 0.592184\n",
      "epoch 22; iter: 0; batch classifier loss: 0.614918\n",
      "epoch 23; iter: 0; batch classifier loss: 0.596635\n",
      "epoch 24; iter: 0; batch classifier loss: 0.624126\n",
      "epoch 25; iter: 0; batch classifier loss: 0.611363\n",
      "epoch 26; iter: 0; batch classifier loss: 0.557795\n",
      "epoch 27; iter: 0; batch classifier loss: 0.519783\n",
      "epoch 28; iter: 0; batch classifier loss: 0.593704\n",
      "epoch 29; iter: 0; batch classifier loss: 0.522679\n",
      "epoch 30; iter: 0; batch classifier loss: 0.548879\n",
      "epoch 31; iter: 0; batch classifier loss: 0.552274\n",
      "epoch 32; iter: 0; batch classifier loss: 0.577358\n",
      "epoch 33; iter: 0; batch classifier loss: 0.525311\n",
      "epoch 34; iter: 0; batch classifier loss: 0.637774\n",
      "epoch 35; iter: 0; batch classifier loss: 0.572835\n",
      "epoch 36; iter: 0; batch classifier loss: 0.577251\n",
      "epoch 37; iter: 0; batch classifier loss: 0.512781\n",
      "epoch 38; iter: 0; batch classifier loss: 0.546854\n",
      "epoch 39; iter: 0; batch classifier loss: 0.561886\n",
      "Train Accuracy:  0.7007677941481635\n",
      "Test Accuracy:  0.6739265712507778\n"
     ]
    }
   ],
   "source": [
    "####### Hyperparameters\n",
    "hyperparameters = {'adversary_loss_weight':0.1, \n",
    "                    'batch_size':64, \n",
    "                    'num_epochs':40, \n",
    "                    'learning_rate':0.001\n",
    "                    }\n",
    "\n",
    "def train_model(scope_name, hyperparameters, debias):\n",
    "    with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)) as sess:\n",
    "        model = AdversarialLogisticModel(scope_name, sess, hyperparameters, seed=1, debias=debias)\n",
    "        trained_model, epoch_losses = model.fit(train_features, train_labels, train_protect)\n",
    "        train_pred_labels = trained_model.predict(train_features, train_labels, train_protect)\n",
    "        test_pred_labels = trained_model.predict(test_features, test_labels, test_protect)\n",
    "    return epoch_losses, train_pred_labels, test_pred_labels\n",
    "\n",
    "epoch_losses, train_pred_labels, test_pred_labels = train_model(\"training\", hyperparameters, False)\n",
    "print(\"Train Accuracy: \", accuracy_score(train_labels, train_pred_labels))\n",
    "print(\"Test Accuracy: \", accuracy_score(test_labels, test_pred_labels))\n",
    "plt.plot(list(range(1, hyperparameters['num_epochs']+1)), epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "black_confusion_matrix = binary_confusion_matrix(test_loader.dataset.label, test_pred_labels, test_loader.dataset.protect, 1)\n",
    "white_confusion_matrix = binary_confusion_matrix(test_loader.dataset.label, test_pred_labels, test_loader.dataset.protect, 0)\n",
    "\n",
    "black_fpr = false_positive_rate(black_confusion_matrix)\n",
    "white_fpr = false_positive_rate(white_confusion_matrix)\n",
    "\n",
    "\n",
    "black_fnr = false_negative_rate(black_confusion_matrix)\n",
    "white_fnr = false_negative_rate(white_confusion_matrix)\n",
    "\n",
    "print(train_accuracy, \"Train Acuuracy\")\n",
    "print(test_accuracy, \"Test Acuuracy\")\n",
    "\n",
    "print(black_confusion_matrix, \" Blacks\")\n",
    "print(white_confusion_matrix, \" Whites\")\n",
    "\n",
    "print(frac_predicted_positive(black_confusion_matrix), frac_predicted_positive(white_confusion_matrix), \"Fraction predicted positive females, males\")\n",
    "print(statistical_parity_difference(black_confusion_matrix, white_confusion_matrix), \" statistical_parity_difference\")\n",
    "\n",
    "print(black_fpr - white_fpr, \"FPR difference\")\n",
    "print(black_fnr - white_fnr, \"FNR difference\")\n",
    "\n",
    "print(true_positive_rate_difference(black_confusion_matrix, white_confusion_matrix), \" true_positive_rate_difference\")\n",
    "print(false_positive_rate_difference(black_confusion_matrix, white_confusion_matrix), \" false_positive_rate_difference\")\n",
    "print(average_odds_difference(black_confusion_matrix, white_confusion_matrix), \" average_odds_difference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
