{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialLogisticModel(object):\n",
    "    \"\"\"A model for doing adversarial training of logistic models.\"\"\"\n",
    "    def __init__(self,  \n",
    "                scope_name,\n",
    "                sess,\n",
    "                hyperparameters,\n",
    "                seed=None,\n",
    "                debias=True):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.adversary_loss_weight = hyperparameters['adversary_loss_weight']\n",
    "        self.num_epochs = hyperparameters[\"num_epochs\"]\n",
    "        self.batch_size = hyperparameters[\"batch_size\"]\n",
    "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
    "        self.debias = debias\n",
    "        \n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def predictor_model(self, features, features_dim):\n",
    "        with tf.variable_scope(\"predictor_model\"):\n",
    "            W1 = tf.get_variable('W1', [self.features_dim, 1], initializer=tf.glorot_uniform_initializer())\n",
    "            b1 = tf.Variable(tf.zeros(shape=[1]), name='b1')\n",
    "\n",
    "            pred_logits = tf.matmul(features, W1) + b1\n",
    "            pred_labels = tf.sigmoid(pred_logits)\n",
    "\n",
    "        return pred_labels, pred_logits\n",
    "\n",
    "    def adversarial_model(self, pred_logits, true_labels):\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1], initializer=tf.glorot_uniform_initializer())\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logits = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_labels = tf.sigmoid(pred_protected_attribute_logits)\n",
    "\n",
    "        return pred_protected_attribute_labels, pred_protected_attribute_logits\n",
    "\n",
    "\n",
    "    def fit(self, features, labels, protect):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self.predictor_model(self.features_ph, self.features_dim)\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self.adversarial_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "            predictor_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            predictor_vars = [var for var in tf.trainable_variables() if 'predictor_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables() if 'adversary_model' in var.name]\n",
    "                # Compute gradient of adversarial loss with respect to predictor variables\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss, var_list=predictor_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "            predictor_grads = []\n",
    "            for (grad,var) in predictor_opt.compute_gradients(pred_labels_loss, var_list=predictor_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                predictor_grads.append((grad, var))\n",
    "            # Update predictor parameters\n",
    "            predictor_minimizer = predictor_opt.apply_gradients(predictor_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = features[batch_ids]\n",
    "                    batch_labels = np.reshape(labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(protect[batch_ids], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                        self.true_labels_ph: batch_labels,\n",
    "                                        self.protected_attributes_ph: batch_protected_attributes\n",
    "                                        }\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([predictor_minimizer, adversary_minimizer,\n",
    "                                       pred_labels_loss, pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch predictor loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run([predictor_minimizer, pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, features, labels, protect):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair classifier learned.\n",
    "        Args:\n",
    "            features\n",
    "            labels\n",
    "            protect\n",
    "        Returns:\n",
    "            predictions\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = features[batch_ids]\n",
    "            batch_labels = np.reshape(labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(protect[batch_ids], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               }\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        true_labels = list(labels.reshape(1, -1)[0])\n",
    "        pred_labels = np.array([1 if i > 0.5 else 0 for i in pred_labels]).reshape(-1,1)\n",
    "        return pred_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filepath):\n",
    "    return pd.read_csv(filepath) \n",
    "\n",
    "def split_train_test(compas_df, train=0.75):\n",
    "    shuffled = np.random.RandomState(0).permutation(compas_df.index)\n",
    "    n_train = int(len(shuffled) * train)\n",
    "    i_train, i_test = shuffled[:n_train], shuffled[n_train:]\n",
    "    return compas_df.loc[i_train], compas_df.loc[i_test]\n",
    "\n",
    "\n",
    "def process_data(df, drop_columns, label_column, protect_column):\n",
    "    df = df.drop(columns=drop_columns, axis=1)\n",
    "    feature_columns = set(df.columns) - {label_column, protect_column}\n",
    "    features = df[list(feature_columns)].values\n",
    "    labels = df[label_column].values.reshape(-1,1)\n",
    "    protect = df[protect_column].values.reshape(-1,1)\n",
    "    return (features, labels, protect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/Compas Dataset/processed_compas.csv' does not exist: b'../data/Compas Dataset/processed_compas.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-90206cec968c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/Compas Dataset/processed_compas.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcompas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# label_column = \"is_recid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# protect_column = \"race_African-American\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d3aba6ee3192>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompas_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompas_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/Compas Dataset/processed_compas.csv' does not exist: b'../data/Compas Dataset/processed_compas.csv'"
     ]
    }
   ],
   "source": [
    "filepath = '../data/Compas Dataset/processed_compas.csv'\n",
    "compas_df = get_data(filepath)\n",
    "# label_column = \"is_recid\"\n",
    "# protect_column = \"race_African-American\"\n",
    "\n",
    "# train_df, test_df = split_train_test(compas_df)\n",
    "# train_features, train_labels, train_protect = process_data(train_df, drop_columns, label_column, protect_column)\n",
    "# test_features, test_labels, test_protect = process_data(test_df, drop_columns, label_column, protect_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "epoch 0; iter: 0; batch predictor loss: 0.860822; batch adversarial loss: 0.677837\n",
      "epoch 1; iter: 0; batch predictor loss: 0.700775; batch adversarial loss: 0.684392\n",
      "epoch 2; iter: 0; batch predictor loss: 0.664454; batch adversarial loss: 0.688395\n",
      "epoch 3; iter: 0; batch predictor loss: 0.603230; batch adversarial loss: 0.683862\n",
      "epoch 4; iter: 0; batch predictor loss: 0.624086; batch adversarial loss: 0.682409\n",
      "epoch 5; iter: 0; batch predictor loss: 0.623858; batch adversarial loss: 0.703794\n",
      "epoch 6; iter: 0; batch predictor loss: 0.653518; batch adversarial loss: 0.662296\n",
      "epoch 7; iter: 0; batch predictor loss: 0.582530; batch adversarial loss: 0.646757\n",
      "epoch 8; iter: 0; batch predictor loss: 0.557741; batch adversarial loss: 0.660530\n",
      "epoch 9; iter: 0; batch predictor loss: 0.604362; batch adversarial loss: 0.677846\n",
      "epoch 10; iter: 0; batch predictor loss: 0.554586; batch adversarial loss: 0.664386\n",
      "epoch 11; iter: 0; batch predictor loss: 0.581680; batch adversarial loss: 0.653459\n",
      "epoch 12; iter: 0; batch predictor loss: 0.538579; batch adversarial loss: 0.682447\n",
      "epoch 13; iter: 0; batch predictor loss: 0.595794; batch adversarial loss: 0.645575\n",
      "epoch 14; iter: 0; batch predictor loss: 0.566684; batch adversarial loss: 0.617607\n",
      "epoch 15; iter: 0; batch predictor loss: 0.642352; batch adversarial loss: 0.664691\n",
      "epoch 16; iter: 0; batch predictor loss: 0.602972; batch adversarial loss: 0.685562\n",
      "epoch 17; iter: 0; batch predictor loss: 0.477891; batch adversarial loss: 0.660173\n",
      "epoch 18; iter: 0; batch predictor loss: 0.610273; batch adversarial loss: 0.586202\n",
      "epoch 19; iter: 0; batch predictor loss: 0.540876; batch adversarial loss: 0.612614\n",
      "epoch 20; iter: 0; batch predictor loss: 0.612361; batch adversarial loss: 0.663983\n",
      "epoch 21; iter: 0; batch predictor loss: 0.555803; batch adversarial loss: 0.642043\n",
      "epoch 22; iter: 0; batch predictor loss: 0.573624; batch adversarial loss: 0.640683\n",
      "epoch 23; iter: 0; batch predictor loss: 0.609635; batch adversarial loss: 0.678471\n",
      "epoch 24; iter: 0; batch predictor loss: 0.573184; batch adversarial loss: 0.662735\n",
      "epoch 25; iter: 0; batch predictor loss: 0.524813; batch adversarial loss: 0.653252\n",
      "epoch 26; iter: 0; batch predictor loss: 0.615040; batch adversarial loss: 0.640645\n",
      "epoch 27; iter: 0; batch predictor loss: 0.592158; batch adversarial loss: 0.614971\n",
      "epoch 28; iter: 0; batch predictor loss: 0.554592; batch adversarial loss: 0.666569\n",
      "epoch 29; iter: 0; batch predictor loss: 0.521103; batch adversarial loss: 0.693454\n",
      "epoch 30; iter: 0; batch predictor loss: 0.609003; batch adversarial loss: 0.694793\n",
      "epoch 31; iter: 0; batch predictor loss: 0.622814; batch adversarial loss: 0.663715\n",
      "epoch 32; iter: 0; batch predictor loss: 0.638887; batch adversarial loss: 0.635250\n",
      "epoch 33; iter: 0; batch predictor loss: 0.677248; batch adversarial loss: 0.644189\n",
      "epoch 34; iter: 0; batch predictor loss: 0.554383; batch adversarial loss: 0.642890\n",
      "epoch 35; iter: 0; batch predictor loss: 0.564012; batch adversarial loss: 0.682913\n",
      "epoch 36; iter: 0; batch predictor loss: 0.506918; batch adversarial loss: 0.665490\n",
      "epoch 37; iter: 0; batch predictor loss: 0.585455; batch adversarial loss: 0.701230\n",
      "epoch 38; iter: 0; batch predictor loss: 0.588763; batch adversarial loss: 0.651939\n",
      "epoch 39; iter: 0; batch predictor loss: 0.554948; batch adversarial loss: 0.658527\n",
      "epoch 40; iter: 0; batch predictor loss: 0.615058; batch adversarial loss: 0.651119\n",
      "epoch 41; iter: 0; batch predictor loss: 0.447713; batch adversarial loss: 0.644954\n",
      "epoch 42; iter: 0; batch predictor loss: 0.450141; batch adversarial loss: 0.666340\n",
      "epoch 43; iter: 0; batch predictor loss: 0.531954; batch adversarial loss: 0.688123\n",
      "epoch 44; iter: 0; batch predictor loss: 0.577564; batch adversarial loss: 0.640926\n",
      "epoch 45; iter: 0; batch predictor loss: 0.620465; batch adversarial loss: 0.636385\n",
      "epoch 46; iter: 0; batch predictor loss: 0.497282; batch adversarial loss: 0.658099\n",
      "epoch 47; iter: 0; batch predictor loss: 0.477511; batch adversarial loss: 0.607674\n",
      "epoch 48; iter: 0; batch predictor loss: 0.546346; batch adversarial loss: 0.639068\n",
      "epoch 49; iter: 0; batch predictor loss: 0.505031; batch adversarial loss: 0.625652\n",
      "Train Accuracy:  0.7390184575761911\n",
      "Test Accuracy:  0.7472103004291846\n"
     ]
    }
   ],
   "source": [
    "####### Hyperparameters\n",
    "hyperparameters = {'adversary_loss_weight':0.1, \n",
    "                    'batch_size':64, \n",
    "                    'num_epochs':50, \n",
    "                    'learning_rate':0.0001\n",
    "                    }\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = AdversarialLogisticModel(\"training\", sess, hyperparameters, seed=1)\n",
    "    trained_model = model.fit(train_features, train_labels, train_protect)\n",
    "    train_pred_labels = trained_model.predict(train_features, train_labels, train_protect)\n",
    "    test_pred_labels = trained_model.predict(test_features, test_labels, test_protect)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(train_labels, train_pred_labels))\n",
    "print(\"Test Accuracy: \", accuracy_score(test_labels, test_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[811  41]\n",
      " [341 200]]  Blacks\n",
      "[[659   4]\n",
      " [203  71]]  Whites\n",
      "0.17012448132780084 0.05333333333333334  FPR blacks, whites\n",
      "0.0929652071916221  statistical_parity_difference\n",
      "0.11367641375059667  average_odds_difference\n"
     ]
    }
   ],
   "source": [
    "def binary_confusion_matrix(true_labels, pred_labels, protect, protect_group):\n",
    "    indices = np.where(protect == protect_group)\n",
    "    group_pred_labels = pred_labels[indices]\n",
    "    group_true_labels = true_labels[indices]\n",
    "\n",
    "    return confusion_matrix(group_true_labels, group_pred_labels)\n",
    "\n",
    "def false_positive_rate(group_confusion_matrix):\n",
    "    return group_confusion_matrix[0][1]/np.sum(group_confusion_matrix[:,1])\n",
    "\n",
    "def true_positive_rate(group_confusion_matrix):\n",
    "    return group_confusion_matrix[1][1]/np.sum(group_confusion_matrix[1,:])\n",
    "\n",
    "def false_negative_rate(group_confusion_matrix):\n",
    "    return group_confusion_matrix[1][0]/np.sum(group_confusion_matrix[:,0])\n",
    "\n",
    "def true_negative_rate(group_confusion_matrix):\n",
    "    return group_confusion_matrix[0][0]/np.sum(group_confusion_matrix[0,:])\n",
    "\n",
    "def false_positive_rate_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "    return false_positive_rate(confusion_matrix_1) - false_positive_rate(confusion_matrix_2) \n",
    "\n",
    "def true_positive_rate_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "    return true_positive_rate(confusion_matrix_1) - true_positive_rate(confusion_matrix_2) \n",
    "\n",
    "def false_negative_rate_difference():\n",
    "    return false_negative_rate(confusion_matrix_1) - false_negative_rate(confusion_matrix_2)\n",
    "\n",
    "def average_odds_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "    fpr_difference = false_positive_rate_difference(confusion_matrix_1, confusion_matrix_2)\n",
    "    tpr_difference = true_positive_rate_difference(confusion_matrix_1, confusion_matrix_2)\n",
    "\n",
    "    return 0.5*(fpr_difference + tpr_difference)\n",
    "\n",
    "def statistical_parity_difference(confusion_matrix_1, confusion_matrix_2):\n",
    "\n",
    "    frac_prediced_positive_1 = np.sum(confusion_matrix_1[:,1])/np.sum(confusion_matrix_1)\n",
    "    frac_prediced_positive_2 = np.sum(confusion_matrix_2[:,1])/np.sum(confusion_matrix_2)\n",
    "\n",
    "    return frac_prediced_positive_1 - frac_prediced_positive_2\n",
    "\n",
    "\n",
    "\n",
    "black_confusion_matrix = binary_confusion_matrix(test_labels, test_pred_labels, test_protect, 1)\n",
    "white_confusion_matrix = binary_confusion_matrix(test_labels, test_pred_labels, test_protect, 0)\n",
    "\n",
    "black_fpr = false_positive_rate(black_confusion_matrix)\n",
    "white_fpr = false_positive_rate(white_confusion_matrix)\n",
    "\n",
    "\n",
    "print(black_confusion_matrix, \" Blacks\")\n",
    "print(white_confusion_matrix, \" Whites\")\n",
    "\n",
    "print(black_fpr, white_fpr, \" FPR blacks, whites\")\n",
    "print(statistical_parity_difference(black_confusion_matrix, white_confusion_matrix), \" statistical_parity_difference\")\n",
    "print(average_odds_difference(black_confusion_matrix, white_confusion_matrix), \" average_odds_difference\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
